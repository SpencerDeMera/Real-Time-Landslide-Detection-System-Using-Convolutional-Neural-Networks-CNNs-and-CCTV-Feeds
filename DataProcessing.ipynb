{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1ETA7EUZIk8lWqo1Tb9UPSSJV_uZSfAXS","authorship_tag":"ABX9TyPxHEyshOUX/DsueIMyef5+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# Data Processing"],"metadata":{"id":"d8q07NPlixtZ"}},{"cell_type":"code","source":["DEBUG = True\n","\n","if DEBUG:\n","    print(\"Debugging Enabled...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAU9_b724jYx","executionInfo":{"status":"ok","timestamp":1740279420612,"user_tz":480,"elapsed":4,"user":{"displayName":"Spencer DeMera","userId":"01380755959708845728"}},"outputId":"1b624034-1a20-41e2-8eaf-9b64d87fceec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Debugging Enabled...\n"]}]},{"cell_type":"markdown","source":["### Process Data Into Global & Categorized CSVs\n","- Splits into a main initialCSVLabels.csv\n","- Splits into 2 sub CSVs:\n","    - initialLabeledTrain.csv\n","    - initialLabeledTest.csv\n","\n"],"metadata":{"id":"83GmXUJCiOR3"}},{"cell_type":"code","source":["import os\n","import random\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","LANDSLIDE_DIR = \"/content/drive/MyDrive/GradProjectFiles/RAW_CCTV_Images/landslides\"\n","NORMAL_DIR = \"/content/drive/MyDrive/GradProjectFiles/RAW_CCTV_Images/standard\"\n","\n","CSV_PATH = \"/content/drive/MyDrive/GradProjectFiles/initialCSVLabels.csv\"\n","TRAIN_CSV_PATH = \"/content/drive/MyDrive/GradProjectFiles/initialLabeledTrain.csv\"\n","TEST_CSV_PATH = \"/content/drive/MyDrive/GradProjectFiles/initialLabeledTest.csv\"\n","\n","# Ensure directories exist\n","os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n","os.makedirs(os.path.dirname(TRAIN_CSV_PATH), exist_ok=True)\n","os.makedirs(os.path.dirname(TEST_CSV_PATH), exist_ok=True)\n","\n","# Create labels\n","data = []\n","\n","if DEBUG:\n","    print(\"Assigning Normal Labels\")\n","# Assign label 0 for normal road images\n","for img_name in os.listdir(NORMAL_DIR):\n","    img_path = os.path.join(NORMAL_DIR, img_name)\n","    if os.path.isfile(img_path):  # Ensure it's a file\n","        data.append({\"file_path\": img_path, \"label\": 0})\n","\n","if DEBUG:\n","    print(\"Assigning Landslide Labels\")\n","# Assign label 1 for landslide images\n","for img_name in os.listdir(LANDSLIDE_DIR):\n","    img_path = os.path.join(LANDSLIDE_DIR, img_name)\n","    if os.path.isfile(img_path):  # Ensure it's a file\n","        data.append({\"file_path\": img_path, \"label\": 1})\n","\n","# Save to CSV\n","df = pd.DataFrame(data)\n","df.to_csv(CSV_PATH, index=False)\n","\n","print(f\"CSV saved to: {CSV_PATH}\")\n","\n","# Split dataset\n","train, test = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Save splits\n","train.to_csv(TRAIN_CSV_PATH, index=False)\n","test.to_csv(TEST_CSV_PATH, index=False)\n","\n","print(f\"Dataset split completed: {TRAIN_CSV_PATH}, {TEST_CSV_PATH}\")"],"metadata":{"id":"8YHeq85HhwFL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740265355296,"user_tz":480,"elapsed":80683,"user":{"displayName":"Spencer DeMera","userId":"01380755959708845728"}},"outputId":"446a69b8-e62e-410f-8ad6-6cbd72381a60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Assigning Normal Labels\n","Assigning Landslide Labels\n","CSV saved to: /content/drive/MyDrive/GradProjectFiles/initialCSVLabels.csv\n","Dataset split completed: /content/drive/MyDrive/GradProjectFiles/initialLabeledTrain.csv, /content/drive/MyDrive/GradProjectFiles/initialLabeledTest.csv\n"]}]},{"cell_type":"markdown","source":["### Break data down into NPZ arrays\n","- Data is broken via batches of 1000s sub-arrays then recombined into single Train, Val, Test arrays"],"metadata":{"id":"7KVAp9sfi17V"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1E06HKpbaPJP","outputId":"8ee3b4ce-255f-4a00-9ee1-ffbcf88cc879","executionInfo":{"status":"ok","timestamp":1740284052465,"user_tz":480,"elapsed":3847649,"user":{"displayName":"Spencer DeMera","userId":"01380755959708845728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Recombine into single numpy arrays\n","Recombining 33 image batches and 33 label batches for train dataset...\n","Batch 1 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 2 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 3 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 4 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 5 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 6 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 7 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 8 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 9 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 10 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 11 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 12 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 13 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 14 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 15 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 16 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 17 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 18 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 19 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 20 - Images: (998, 260, 320, 3), Labels: (998, 2)\n","Batch 21 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 22 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 23 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 24 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 25 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 26 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 27 - Images: (380, 260, 320, 3), Labels: (380, 2)\n","Batch 28 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 29 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 30 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 31 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 32 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 33 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Final combined shape - Images: (32365, 260, 320, 3), Labels: (32365, 2)\n","Successfully recombined and saved train dataset separately:\n","Images: (32365, 260, 320, 3), Labels: (32365, 2)\n","Recombining 9 image batches and 9 label batches for test dataset...\n","Batch 1 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 2 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 3 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 4 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 5 - Images: (999, 260, 320, 3), Labels: (999, 2)\n","Batch 6 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 7 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 8 - Images: (1000, 260, 320, 3), Labels: (1000, 2)\n","Batch 9 - Images: (94, 260, 320, 3), Labels: (94, 2)\n","Final combined shape - Images: (8092, 260, 320, 3), Labels: (8092, 2)\n","Successfully recombined and saved test dataset separately:\n","Images: (8092, 260, 320, 3), Labels: (8092, 2)\n","Generating validation images and labels...\n","Saving numpy array data...\n","Train: (25892, 260, 320, 3), (25892, 2)\n","Validation: (6473, 260, 320, 3), (6473, 2)\n","Test: (8092, 260, 320, 3), (8092, 2)\n"]}],"source":["import os\n","import gc\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","\n","CSV_PATH = '/content/drive/MyDrive/GradProjectFiles/initialCSVLabels.csv'\n","TRAIN_CSV_PATH = '/content/drive/MyDrive/GradProjectFiles/initialLabeledTrain.csv'\n","TEST_CSV_PATH = '/content/drive/MyDrive/GradProjectFiles/initialLabeledTest.csv'\n","\n","numpySaves = \"/content/drive/MyDrive/GradProjectFiles/NumPyDataSaves\"\n","\n","ImgSize = (320,260)\n","NumClasses = 2  # Binary classification: normal (0) vs landslide (1)\n","BatchSize = 128\n","EpochNum = 50\n","BatchSize = 1000  # Number of images to process per batch (adjust based on memory capacity)\n","\n","trainDataFrame = pd.read_csv(TRAIN_CSV_PATH)\n","testDataFrame = pd.read_csv(TEST_CSV_PATH)\n","\n","os.makedirs(numpySaves, exist_ok=True)\n","\n","def loadAndPreprocessImage(path):\n","    try:\n","        img = Image.open(path).convert(\"RGB\")  # Load image with PIL\n","        img = img.resize(ImgSize)\n","        img = np.array(img, dtype=np.float32) / 255.0  # Normalize\n","        return img\n","    except Exception as e:\n","        print(f\"Skipping invalid image: {path} - Error: {e}\")\n","        return None\n","\n","\n","# Batch Processing Function\n","def process_and_save_batches(df, dataset_type, batch_size):\n","    total_images = len(df)\n","    num_batches = (total_images // batch_size) + (1 if total_images % batch_size != 0 else 0)\n","\n","    if DEBUG:\n","        print(f\"Processing {dataset_type} dataset in {num_batches} batches of {batch_size} images each...\")\n","\n","    for batch_idx in range(num_batches):\n","        start_idx = batch_idx * batch_size\n","        end_idx = min(start_idx + batch_size, total_images)\n","\n","        batch_images = []\n","        batch_labels = []\n","\n","        for i in range(start_idx, end_idx):\n","            img = loadAndPreprocessImage(df.iloc[i][\"file_path\"])\n","            if img is not None:\n","                batch_images.append(img)\n","                batch_labels.append(df.iloc[i][\"label\"])\n","\n","        if batch_images:\n","            batch_images = np.array(batch_images)\n","            batch_labels = np.array(batch_labels)\n","            batch_labels = to_categorical(batch_labels, num_classes=NumClasses)\n","\n","            # Print the shape of the data in each batch\n","            if DEBUG:\n","                print(f\"Shape of {dataset_type} batch {batch_idx+1} images: {batch_images.shape}\")\n","                print(f\"Shape of {dataset_type} batch {batch_idx+1} labels: {batch_labels.shape}\")\n","\n","            # Save the batch in compressed format to reduce disk usage\n","            np.savez_compressed(os.path.join(numpySaves, f\"{dataset_type}Images_batch{batch_idx}.npz\"), batch_images)\n","            np.savez_compressed(os.path.join(numpySaves, f\"{dataset_type}Labels_batch{batch_idx}.npz\"), batch_labels)\n","\n","            if DEBUG:\n","                print(f\"Saved batch {batch_idx+1}/{num_batches} for {dataset_type}\")\n","\n","            # Free up memory\n","            del batch_images, batch_labels\n","            gc.collect()\n","\n","def recombine_numpy_batches(dataset_type):\n","    image_batches = sorted([f for f in os.listdir(numpySaves) if f.startswith(f\"{dataset_type}Images_batch\") and f.endswith(\".npz\")])\n","    label_batches = sorted([f for f in os.listdir(numpySaves) if f.startswith(f\"{dataset_type}Labels_batch\") and f.endswith(\".npz\")])\n","\n","    if len(image_batches) == 0 or len(label_batches) == 0:\n","        print(f\"Error: No batch files found for {dataset_type}. Check if they were saved correctly.\")\n","        return\n","\n","    if DEBUG:\n","        print(f\"Recombining {len(image_batches)} image batches and {len(label_batches)} label batches for {dataset_type} dataset...\")\n","\n","    final_images = []\n","    final_labels = []\n","\n","    for i, (img_batch, lbl_batch) in enumerate(zip(image_batches, label_batches)):\n","        img_path = os.path.join(numpySaves, img_batch)\n","        lbl_path = os.path.join(numpySaves, lbl_batch)\n","\n","        # Load compressed numpy arrays properly\n","        with np.load(img_path) as img_data:\n","            img_array = img_data[\"arr_0\"]  # Extract the array correctly\n","\n","        with np.load(lbl_path) as lbl_data:\n","            lbl_array = lbl_data[\"arr_0\"]  # Extract the array correctly\n","\n","        if DEBUG:\n","            print(f\"Batch {i+1} - Images: {img_array.shape}, Labels: {lbl_array.shape}\")\n","\n","        final_images.append(img_array)\n","        final_labels.append(lbl_array)\n","\n","        # Free memory\n","        del img_array, lbl_array\n","        gc.collect()\n","\n","    # Concatenate along the first dimension (batch size)\n","    final_images = np.concatenate(final_images, axis=0)\n","    final_labels = np.concatenate(final_labels, axis=0)\n","\n","    if DEBUG:\n","        print(f\"Final combined shape - Images: {final_images.shape}, Labels: {final_labels.shape}\")\n","\n","    # Save images and labels separately\n","    np.savez_compressed(os.path.join(numpySaves, f\"{dataset_type}Images.npz\"), final_images)\n","    np.savez_compressed(os.path.join(numpySaves, f\"{dataset_type}Labels.npz\"), final_labels)\n","\n","    print(f\"Successfully recombined and saved {dataset_type} dataset separately:\\n\"\n","          f\"Images: {final_images.shape}, Labels: {final_labels.shape}\")\n","\n","    # Free memory\n","    del final_images, final_labels\n","    gc.collect()\n","\n","# Process training data in batches\n","# process_and_save_batches(trainDataFrame, \"train\", BatchSize)\n","# Process test data in batches\n","# process_and_save_batches(testDataFrame, \"test\", BatchSize)\n","\n","# Recombine batches into a single file\n","if DEBUG:\n","    print(\"Recombine into single numpy arrays\")\n","recombine_numpy_batches(\"train\")\n","recombine_numpy_batches(\"test\")\n","\n","# Load the final `.npz` dataset\n","trainImages = np.load(os.path.join(numpySaves, \"trainImages.npz\"))[\"arr_0\"]\n","trainLabels = np.load(os.path.join(numpySaves, \"trainLabels.npz\"))[\"arr_0\"]\n","testImages = np.load(os.path.join(numpySaves, \"testImages.npz\"))[\"arr_0\"]\n","testLabels = np.load(os.path.join(numpySaves, \"testLabels.npz\"))[\"arr_0\"]\n","\n","# Split training data further into train and validation sets\n","if DEBUG:\n","    print(\"Generating validation images and labels...\")\n","trainImages, valImages, trainLabels, valLabels = train_test_split(trainImages, trainLabels, test_size=0.2, random_state=42)\n","\n","if DEBUG:\n","    print(\"Saving numpy array data...\")\n","np.savez_compressed(os.path.join(numpySaves, \"trainImages.npz\"), trainImages)\n","np.savez_compressed(os.path.join(numpySaves, \"trainLabels.npz\"), trainLabels)\n","np.savez_compressed(os.path.join(numpySaves, \"valImages.npz\"), valImages)\n","np.savez_compressed(os.path.join(numpySaves, \"valLabels.npz\"), valLabels)\n","np.savez_compressed(os.path.join(numpySaves, \"testImages.npz\"), testImages)\n","np.savez_compressed(os.path.join(numpySaves, \"testLabels.npz\"), testLabels)\n","\n","if DEBUG:\n","    print(f\"Train: {trainImages.shape}, {trainLabels.shape}\")\n","    print(f\"Validation: {valImages.shape}, {valLabels.shape}\")\n","    print(f\"Test: {testImages.shape}, {testLabels.shape}\")\n"]}]}